{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Task2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1dB1FArCSFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import wordnet as wn\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.wsd import lesk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toWmkAxzDNc0",
        "colab_type": "code",
        "outputId": "3288dcf4-47ed-4f5b-8e8e-a32b53be84ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "nltk.download('all')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQcLrThV5tSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def penn_to_wn(tag):\n",
        "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
        "    if tag.startswith('N'):\n",
        "        return 'n'\n",
        " \n",
        "    if tag.startswith('V'):\n",
        "        return 'v'\n",
        " \n",
        "    if tag.startswith('J'):\n",
        "        return 'a'\n",
        " \n",
        "    if tag.startswith('R'):\n",
        "        return 'r'\n",
        " \n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4rjh-ruG6WD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def task2(sentence):\n",
        "\n",
        "  #loading spacy mode\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "  #Tokenization\n",
        "  tokens = []\n",
        "  tokens = nltk.word_tokenize(sentence);\n",
        "  print(\"\\n Tokens: \\n\", tokens)\n",
        "\n",
        "  #Lemmatization without POS tags\n",
        "  lems = []\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  for word in tokens:\n",
        "    lems.append(lemmatizer.lemmatize(word));\n",
        "  print(\"\\n Lemmatization of sentence without POS tags: \\n\",lems)\n",
        "\n",
        "  # POS Tagging\n",
        "  pos_sen = pos_tag(tokens);\n",
        "  print(\"\\n POS Tags: \\n\",pos_sen);\n",
        "\n",
        "  pos_wn = [(s[0],penn_to_wn(s[1])) for s in pos_sen]  \n",
        "  print(\"\\n POS Tags for wordnet: \\n\",pos_wn)\n",
        "\n",
        "  #Lemmatization with POS tags\n",
        "  lems_pos = []\n",
        "  for w in pos_wn:\n",
        "    if(w[1]):\n",
        "      lems_pos.append(lemmatizer.lemmatize(w[0],pos=w[1]))\n",
        "    else:\n",
        "      lems_pos.append(lemmatizer.lemmatize(w[0]))\n",
        "  print(\"\\n Lemmatization by taking into account the pos tags: \\n\")\n",
        "  print(lems_pos)\n",
        "\n",
        "  #Dependency Parse Tree\n",
        "  sen = nlp(sentence)\n",
        "  print(\"\\n Dependency parse tree: \\n\")\n",
        "  for token in sen:\n",
        "    print(\"{2}({3}-{6}, {0}-{5})\".format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_, token.i+1, token.head.i+1))\n",
        "\n",
        "  #Finding the synsets of the words:\n",
        "  synset_sen = []\n",
        "  synset_sen = [wn.synsets(s) for s in sentence]\n",
        "  print(\"\\n The synsets of all the words in the sentence: \\n\")\n",
        "  [print(s,\" : \",wn.synsets(s)) for s in tokens]\n",
        "  \n",
        "\n",
        "  #Word Sense Disambiguation \n",
        "  word_sense = []\n",
        "  for w in pos_wn:\n",
        "    if(w[1]):\n",
        "      word_sense.append((w[0],lesk(sentence,w[0],pos=w[1])))\n",
        "    else:\n",
        "      word_sense.append((w[0],lesk(sentence,w[0])))\n",
        "\n",
        "  print(\"\\n disambiguated word senses: \\n\")\n",
        "  print(word_sense)\n",
        "\n",
        "  #Finding Relations\n",
        "\n",
        "  hypernyms_sen = []\n",
        "  hyponyms_sen = []\n",
        "  holonyms_part_sen = []\n",
        "  holonyms_substance_sen = []\n",
        "  meronyms_part_sen = []\n",
        "  meronyms_substance_sen = []\n",
        "\n",
        "  for w in word_sense:\n",
        "    if(w[1]):\n",
        "      \n",
        "      hypernyms_sen.append((w[0],w[1].hypernyms()))\n",
        "      hyponyms_sen.append((w[0],w[1].hyponyms()))\n",
        "      holonyms_part_sen.append((w[0],w[1].part_holonyms()))\n",
        "      holonyms_substance_sen.append((w[0],w[1].substance_holonyms()))\n",
        "      meronyms_part_sen.append((w[0],w[1].part_meronyms()))\n",
        "      meronyms_substance_sen.append((w[0],w[1].substance_meronyms()))\n",
        "\n",
        "    else:\n",
        "      hypernyms_sen.append((w[0],None))\n",
        "      hyponyms_sen.append((w[0],None))\n",
        "      holonyms_part_sen.append((w[0],None))\n",
        "      holonyms_substance_sen.append((w[0],None))\n",
        "      meronyms_part_sen.append((w[0],None))\n",
        "      meronyms_substance_sen.append((w[0],None))\n",
        "\n",
        "\n",
        "  print(\"\\n Hypernyms: \\n\")\n",
        "  print(hypernyms_sen)\n",
        "\n",
        "  print(\"\\n Hyponyms: \\n\")\n",
        "  print(hyponyms_sen)\n",
        "\n",
        "  print(\"\\n part_Holonyms: \\n\")\n",
        "  print(holonyms_part_sen)\n",
        "\n",
        "  print(\"\\n substance_Holonyms: \\n\")\n",
        "  print(holonyms_substance_sen)\n",
        "\n",
        "  print(\"\\n part_Meronyms: \\n\")\n",
        "  print(meronyms_part_sen)\n",
        "\n",
        "  print(\"\\n substance_Meronyms: \\n\")\n",
        "  print(meronyms_substance_sen)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        " \n",
        "   \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr061PXD8K63",
        "colab_type": "code",
        "outputId": "9d0af228-cc59-489c-b8cd-f2520ff3644b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "task2(\"My name is Divya Sharma and I love fruits and table and cycle handle\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Tokens: \n",
            " ['My', 'name', 'is', 'Divya', 'Sharma', 'and', 'I', 'love', 'fruits', 'and', 'table', 'and', 'cycle', 'handle']\n",
            "\n",
            " Lemmatization of sentence without POS tags: \n",
            " ['My', 'name', 'is', 'Divya', 'Sharma', 'and', 'I', 'love', 'fruit', 'and', 'table', 'and', 'cycle', 'handle']\n",
            "\n",
            " POS Tags: \n",
            " [('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Divya', 'NNP'), ('Sharma', 'NNP'), ('and', 'CC'), ('I', 'PRP'), ('love', 'VBP'), ('fruits', 'NNS'), ('and', 'CC'), ('table', 'JJ'), ('and', 'CC'), ('cycle', 'NN'), ('handle', 'NN')]\n",
            "\n",
            " POS Tags for wordnet: \n",
            " [('My', None), ('name', 'n'), ('is', 'v'), ('Divya', 'n'), ('Sharma', 'n'), ('and', None), ('I', None), ('love', 'v'), ('fruits', 'n'), ('and', None), ('table', 'a'), ('and', None), ('cycle', 'n'), ('handle', 'n')]\n",
            "\n",
            " Lemmatization by taking into account the pos tags: \n",
            "\n",
            "['My', 'name', 'be', 'Divya', 'Sharma', 'and', 'I', 'love', 'fruit', 'and', 'table', 'and', 'cycle', 'handle']\n",
            "\n",
            " Dependency parse tree: \n",
            "\n",
            "poss(name-2, My-1)\n",
            "nsubj(is-3, name-2)\n",
            "ROOT(is-3, is-3)\n",
            "compound(Sharma-5, Divya-4)\n",
            "attr(is-3, Sharma-5)\n",
            "cc(love-8, and-6)\n",
            "nsubj(love-8, I-7)\n",
            "ROOT(love-8, love-8)\n",
            "dobj(love-8, fruits-9)\n",
            "cc(fruits-9, and-10)\n",
            "nmod(handle-14, table-11)\n",
            "cc(table-11, and-12)\n",
            "conj(table-11, cycle-13)\n",
            "conj(fruits-9, handle-14)\n",
            "\n",
            " The synsets of all the words in the sentence: \n",
            "\n",
            "My  :  []\n",
            "name  :  [Synset('name.n.01'), Synset('name.n.02'), Synset('name.n.03'), Synset('name.n.04'), Synset('name.n.05'), Synset('name.n.06'), Synset('name.v.01'), Synset('name.v.02'), Synset('name.v.03'), Synset('appoint.v.01'), Synset('name.v.05'), Synset('mention.v.01'), Synset('identify.v.05'), Synset('list.v.01'), Synset('diagnose.v.01')]\n",
            "is  :  [Synset('be.v.01'), Synset('be.v.02'), Synset('be.v.03'), Synset('exist.v.01'), Synset('be.v.05'), Synset('equal.v.01'), Synset('constitute.v.01'), Synset('be.v.08'), Synset('embody.v.02'), Synset('be.v.10'), Synset('be.v.11'), Synset('be.v.12'), Synset('cost.v.01')]\n",
            "Divya  :  []\n",
            "Sharma  :  []\n",
            "and  :  []\n",
            "I  :  [Synset('iodine.n.01'), Synset('one.n.01'), Synset('i.n.03'), Synset('one.s.01')]\n",
            "love  :  [Synset('love.n.01'), Synset('love.n.02'), Synset('beloved.n.01'), Synset('love.n.04'), Synset('love.n.05'), Synset('sexual_love.n.02'), Synset('love.v.01'), Synset('love.v.02'), Synset('love.v.03'), Synset('sleep_together.v.01')]\n",
            "fruits  :  [Synset('fruit.n.01'), Synset('yield.n.03'), Synset('fruit.n.03'), Synset('fruit.v.01'), Synset('fruit.v.02')]\n",
            "and  :  []\n",
            "table  :  [Synset('table.n.01'), Synset('table.n.02'), Synset('table.n.03'), Synset('mesa.n.01'), Synset('table.n.05'), Synset('board.n.04'), Synset('postpone.v.01'), Synset('table.v.02')]\n",
            "and  :  []\n",
            "cycle  :  [Synset('cycle.n.01'), Synset('cycle.n.02'), Synset('cycle.n.03'), Synset('hertz.n.01'), Synset('cycle.n.05'), Synset('bicycle.n.01'), Synset('cycle.v.01'), Synset('cycle.v.02'), Synset('motorbike.v.01'), Synset('bicycle.v.01'), Synset('cycle.v.05')]\n",
            "handle  :  [Synset('handle.n.01'), Synset('manage.v.02'), Synset('treat.v.01'), Synset('cover.v.05'), Synset('handle.v.04'), Synset('wield.v.02'), Synset('handle.v.06')]\n",
            "\n",
            " disambiguated word senses: \n",
            "\n",
            "[('My', None), ('name', Synset('name.n.06')), ('is', Synset('embody.v.02')), ('Divya', None), ('Sharma', None), ('and', None), ('I', Synset('one.s.01')), ('love', Synset('love.v.01')), ('fruits', Synset('yield.n.03')), ('and', None), ('table', None), ('and', None), ('cycle', Synset('hertz.n.01')), ('handle', Synset('handle.n.01'))]\n",
            "\n",
            " Hypernyms: \n",
            "\n",
            "[('My', None), ('name', [Synset('defamation.n.01')]), ('is', [Synset('typify.v.02')]), ('Divya', None), ('Sharma', None), ('and', None), ('I', []), ('love', []), ('fruits', [Synset('product.n.02')]), ('and', None), ('table', None), ('and', None), ('cycle', [Synset('rate.n.01')]), ('handle', [Synset('appendage.n.03')])]\n",
            "\n",
            " Hyponyms: \n",
            "\n",
            "[('My', None), ('name', [Synset('smear_word.n.01')]), ('is', [Synset('body.v.01'), Synset('exemplify.v.01')]), ('Divya', None), ('Sharma', None), ('and', None), ('I', []), ('love', [Synset('adore.v.01'), Synset('care_for.v.02'), Synset('dote.v.02'), Synset('love.v.03')]), ('fruits', []), ('and', None), ('table', None), ('and', None), ('cycle', []), ('handle', [Synset('ax_handle.n.01'), Synset('broomstick.n.01'), Synset('crop.n.05'), Synset('haft.n.01'), Synset('hilt.n.01'), Synset('hoe_handle.n.01'), Synset('knob.n.02'), Synset('mop_handle.n.01'), Synset('panhandle.n.02'), Synset('pommel.n.01'), Synset('pommel.n.02'), Synset('rake_handle.n.01'), Synset('stock.n.03'), Synset('stock.n.15')])]\n",
            "\n",
            " part_Holonyms: \n",
            "\n",
            "[('My', None), ('name', []), ('is', []), ('Divya', None), ('Sharma', None), ('and', None), ('I', []), ('love', []), ('fruits', []), ('and', None), ('table', None), ('and', None), ('cycle', [Synset('kilohertz.n.01')]), ('handle', [Synset('aspergill.n.01'), Synset('baggage.n.01'), Synset('baseball_bat.n.01'), Synset('briefcase.n.01'), Synset('brush.n.02'), Synset('carpet_beater.n.01'), Synset('carrycot.n.01'), Synset('cheese_cutter.n.01'), Synset('coffee_cup.n.01'), Synset('coffeepot.n.01'), Synset('cricket_bat.n.01'), Synset('cutlery.n.02'), Synset('edge_tool.n.01'), Synset('faucet.n.01'), Synset('frying_pan.n.01'), Synset('hand_tool.n.01'), Synset('handbarrow.n.01'), Synset('handcart.n.01'), Synset('handlebar.n.01'), Synset('handset.n.01'), Synset('ladle.n.01'), Synset('mug.n.04'), Synset('racket.n.04'), Synset('saucepan.n.01'), Synset('spatula.n.01'), Synset('teacup.n.02'), Synset('umbrella.n.01'), Synset('watering_can.n.01')])]\n",
            "\n",
            " substance_Holonyms: \n",
            "\n",
            "[('My', None), ('name', []), ('is', []), ('Divya', None), ('Sharma', None), ('and', None), ('I', []), ('love', []), ('fruits', []), ('and', None), ('table', None), ('and', None), ('cycle', []), ('handle', [])]\n",
            "\n",
            " part_Meronyms: \n",
            "\n",
            "[('My', None), ('name', []), ('is', []), ('Divya', None), ('Sharma', None), ('and', None), ('I', []), ('love', []), ('fruits', []), ('and', None), ('table', None), ('and', None), ('cycle', []), ('handle', [Synset('shank.n.03')])]\n",
            "\n",
            " substance_Meronyms: \n",
            "\n",
            "[('My', None), ('name', []), ('is', []), ('Divya', None), ('Sharma', None), ('and', None), ('I', []), ('love', []), ('fruits', []), ('and', None), ('table', None), ('and', None), ('cycle', []), ('handle', [])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1NuZgkWVU1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}